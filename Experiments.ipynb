{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "from progressbar import ProgressBar\n",
    "import cPickle as pkl\n",
    "\n",
    "from DependencyTree import DependencyTree\n",
    "from ParsingSystem import ParsingSystem\n",
    "from Configuration import Configuration\n",
    "import Config\n",
    "import Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDictionaries(sents, trees):\n",
    "    word = []\n",
    "    pos = []\n",
    "    label = []\n",
    "    for s in sents:\n",
    "        for token in s:\n",
    "            word.append(token['word'])\n",
    "            pos.append(token['POS'])\n",
    "\n",
    "    rootLabel = None\n",
    "    for tree in trees:\n",
    "        for k in range(1, tree.n + 1):\n",
    "            if tree.getHead(k) == 0:\n",
    "                rootLabel = tree.getLabel(k)\n",
    "            else:\n",
    "                label.append(tree.getLabel(k))\n",
    "\n",
    "    if rootLabel in label:\n",
    "        label.remove(rootLabel)\n",
    "\n",
    "    index = 0\n",
    "    wordCount = [Config.UNKNOWN, Config.NULL, Config.ROOT]\n",
    "    wordCount.extend(collections.Counter(word))\n",
    "    for word in wordCount:\n",
    "        wordDict[word] = index\n",
    "        index += 1\n",
    "\n",
    "    posCount = [Config.UNKNOWN, Config.NULL, Config.ROOT]\n",
    "    posCount.extend(collections.Counter(pos))\n",
    "    for pos in posCount:\n",
    "        posDict[pos] = index\n",
    "        index += 1\n",
    "\n",
    "    labelCount = [Config.NULL, rootLabel]\n",
    "    labelCount.extend(collections.Counter(label))\n",
    "    for label in labelCount:\n",
    "        labelDict[label] = index\n",
    "        index += 1\n",
    "\n",
    "    return wordDict, posDict, labelDict\n",
    "\n",
    "\n",
    "def getWordID(s):\n",
    "    if s in wordDict:\n",
    "        return wordDict[s]\n",
    "    else:\n",
    "        return wordDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getPosID(s):\n",
    "    if s in posDict:\n",
    "        return posDict[s]\n",
    "    else:\n",
    "        return posDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getLabelID(s):\n",
    "    if s in labelDict:\n",
    "        return labelDict[s]\n",
    "    else:\n",
    "        return labelDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getFeatures(c):\n",
    "\n",
    "    \"\"\"\n",
    "    =================================================================\n",
    "\n",
    "    Implement feature extraction described in\n",
    "    \"A Fast and Accurate Dependency Parser using Neural Networks\"(2014)\n",
    "\n",
    "    =================================================================\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for i in range(3):\n",
    "        word_ind = c.getStack(i)\n",
    "        word = c.getWord(word_ind)\n",
    "        word_id = getWordID(word)\n",
    "\n",
    "        features.append(word_id)\n",
    "    \n",
    "    for i in range(3):\n",
    "        word_ind = c.getBuffer(i)\n",
    "        word = c.getWord(word_ind)\n",
    "        word_id = getWordID(word)\n",
    "\n",
    "        features.append(word_id)\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(1,3):\n",
    "            word_ind = c.getLeftChild(c.getStack(i),j)\n",
    "            word = c.getWord(word_ind)\n",
    "            word_id = getWordID(word)\n",
    "\n",
    "            features.append(word_id)\n",
    "\n",
    "            word_ind = c.getRightChild(c.getStack(i),j)\n",
    "            word = c.getWord(word_ind)\n",
    "            word_id = getWordID(word)\n",
    "\n",
    "            features.append(word_id)\n",
    "\n",
    "    for i in range(2):\n",
    "        word_ind = c.getLeftChild(c.getLeftChild(c.getStack(i),1),1)\n",
    "\n",
    "        word = c.getWord(word_ind)\n",
    "        word_id = getWordID(word)\n",
    "\n",
    "        features.append(word_id)\n",
    "\n",
    "        word_ind = c.getRightChild(c.getRightChild(c.getStack(i),1),1)\n",
    "\n",
    "        word = c.getWord(word_ind)\n",
    "        word_id = getWordID(word)\n",
    "\n",
    "        features.append(word_id)\n",
    "\n",
    "    pos_features = []\n",
    "    for ind in features:\n",
    "        pos = c.getPOS(ind)\n",
    "        pos_id = getPosID(pos)\n",
    "\n",
    "        pos_features.append(pos_id)\n",
    "\n",
    "    label_features = []\n",
    "    for ind in features[6:]: # excluding the 3 words from the top of stack and top of buffer\n",
    "        label = c.getLabel(ind)\n",
    "        if label == Config.UNKNOWN:\n",
    "            label = Config.NULL\n",
    "        label_id = getLabelID(label)\n",
    "\n",
    "        label_features.append(label_id)\n",
    "    \n",
    "    features.extend(pos_features)\n",
    "    features.extend(label_features)\n",
    "\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def genTrainExamples(sents, trees):\n",
    "    numTrans = parsing_system.numTransitions()\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    pbar = ProgressBar()\n",
    "    for i in pbar(range(len(sents))):\n",
    "    # for i in pbar(range(11)):\n",
    "\n",
    "        if trees[i].isProjective():\n",
    "            c = parsing_system.initialConfiguration(sents[i])\n",
    "\n",
    "            while not parsing_system.isTerminal(c):\n",
    "                oracle = parsing_system.getOracle(c, trees[i])\n",
    "                feat = getFeatures(c)\n",
    "                label = []\n",
    "                for j in range(numTrans):\n",
    "                    t = parsing_system.transitions[j]\n",
    "                    if t == oracle:\n",
    "                        label.append(1.)\n",
    "                    elif parsing_system.canApply(c, t):\n",
    "                        label.append(0.)\n",
    "                    else:\n",
    "                        label.append(-1.)\n",
    "\n",
    "                if 1.0 not in label:\n",
    "                    print i, label\n",
    "                features.append(feat)\n",
    "                labels.append(label)\n",
    "                c = parsing_system.apply(c, oracle)\n",
    "            \n",
    "#             print(trees[i].equal(c.tree))\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def load_embeddings(filename, wordDict, posDict, labelDict):\n",
    "    dictionary, word_embeds = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    embedding_array = np.zeros((len(wordDict) + len(posDict) + len(labelDict), Config.embedding_size))\n",
    "    pos_ind = 0\n",
    "    dep_ind = 0\n",
    "    knownWords = wordDict.keys()\n",
    "    foundEmbed = 0\n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        if i < len(knownWords):\n",
    "            w = knownWords[i]\n",
    "            if w in dictionary:\n",
    "                index = dictionary[w]\n",
    "            elif w.lower() in dictionary:\n",
    "                index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "\n",
    "        else:\n",
    "            #for unknown words, pos and labels\n",
    "            embedding_array[i] = np.random.rand(Config.embedding_size) * 0.02 - 0.01            \n",
    "          \n",
    "#         elif i < len(wordDict):\n",
    "#             #for unknown words\n",
    "#             embedding_array[i] = np.random.rand(Config.embedding_size) * 0.02 - 0.01            \n",
    "        \n",
    "#         elif pos_ind < len(posDict):\n",
    "#             dummy_emb = [0 for _ in range(Config.embedding_size)]\n",
    "#             dummy_emb[pos_ind] = 1\n",
    "#             embedding_array[i] = dummy_emb[:]\n",
    "#             pos_ind+=1\n",
    "#         else:\n",
    "#             dummy_emb = [0 for _ in range(Config.embedding_size)]\n",
    "#             dummy_emb[dep_ind] = 1\n",
    "#             embedding_array[i] = dummy_emb[:]\n",
    "#             dep_ind+=1\n",
    "            \n",
    "    print \"Found embeddings: \", foundEmbed, \"/\", len(knownWords)\n",
    "\n",
    "    return embedding_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings:  30160 / 44392\n",
      "root\n",
      "Generating Training Examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28% |####################                                                    |\r"
     ]
    }
   ],
   "source": [
    "wordDict = {}\n",
    "posDict = {}\n",
    "labelDict = {}\n",
    "parsing_system = None\n",
    "\n",
    "trainSents, trainTrees = Util.loadConll('train.conll')\n",
    "devSents, devTrees = Util.loadConll('dev.conll')\n",
    "testSents, _ = Util.loadConll('test.conll')\n",
    "genDictionaries(trainSents, trainTrees)\n",
    "\n",
    "embedding_filename = 'word2vec.model'\n",
    "\n",
    "embedding_array = load_embeddings(embedding_filename, wordDict, posDict, labelDict)\n",
    "\n",
    "labelInfo = []\n",
    "for idx in np.argsort(labelDict.values()):\n",
    "    labelInfo.append(labelDict.keys()[idx])\n",
    "parsing_system = ParsingSystem(labelInfo[1:])\n",
    "print parsing_system.rootLabel\n",
    "\n",
    "print \"Generating Training Examples\"\n",
    "trainFeats, trainLabels = genTrainExamples(trainSents, trainTrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script defines a transition-based dependency parser which makes\n",
    "use of a classifier powered by a neural network. The neural network\n",
    "accepts distributed representation inputs: dense, continuous\n",
    "representations of words, their part of speech tags, and the labels\n",
    "which connect words in a partial dependency parse.\n",
    "\n",
    "This is an implementation of the method described in\n",
    "\n",
    "Danqi Chen and Christopher Manning. A Fast and Accurate Dependency Parser Using Neural Networks. In EMNLP 2014.\n",
    "\n",
    "Author: Danqi Chen, Jon Gauthier\n",
    "Modified by: Heeyoung Kwon (2017)\n",
    "Modified by: Jun S. Kang (2018 Mar)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DependencyParserModel(object):\n",
    "\n",
    "    def __init__(self, graph, embedding_array, Config):\n",
    "\n",
    "        self.build_graph(graph, embedding_array, Config)\n",
    "\n",
    "    def build_graph(self, graph, embedding_array, Config):\n",
    "        \"\"\"\n",
    "\n",
    "        :param graph:\n",
    "        :param embedding_array:\n",
    "        :param Config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        with graph.as_default():\n",
    "            \"\"\"\n",
    "            ===================================================================\n",
    "\n",
    "            Define the computational graph with necessary variables.\n",
    "            \n",
    "            1) You may need placeholders of:\n",
    "                - Many parameters are defined at Config: batch_size, n_Tokens, etc\n",
    "                - # of transitions can be get by calling parsing_system.numTransitions()\n",
    "                \n",
    "            self.train_inputs = \n",
    "            self.train_labels = \n",
    "            self.test_inputs =\n",
    "            ...\n",
    "            \n",
    "                \n",
    "            2) Call forward_pass and get predictions\n",
    "            \n",
    "            ...\n",
    "            self.prediction = self.forward_pass(embed, weights_input, biases_input, weights_output)\n",
    "\n",
    "\n",
    "            3) Implement the loss function described in the paper\n",
    "             - lambda is defined at Config.lam\n",
    "            \n",
    "            ...\n",
    "            self.loss =\n",
    "            \n",
    "            ===================================================================\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            self.embeddings = tf.Variable(embedding_array, dtype=tf.float32,name='all_embeddings', trainable=False)\n",
    "#             self.batch_size = tf.constant(Config.batch_size, \"int32\")\n",
    "#             self.embedding_size = tf.constant(Config.embedding_size, \"int32\")\n",
    "\n",
    "            feature_len = 18 + 18 + 12\n",
    "            numTrans = parsing_system.numTransitions()\n",
    "\n",
    "            self.train_inputs = tf.placeholder(\"int32\",[Config.batch_size,feature_len], name= 'train_input')\n",
    "            self.train_labels = tf.placeholder(\"int32\",[Config.batch_size,numTrans], name= 'train_labels')\n",
    "            self.train_embed = tf.nn.embedding_lookup(self.embeddings, self.train_inputs, name = 'train_embed')\n",
    "\n",
    "            ### weights and biases\n",
    "            weights_input = tf.Variable(tf.random.truncated_normal([18*Config.embedding_size, Config.hidden_size],stddev=0.008), name='w_in')\n",
    "            biases_input = tf.Variable(tf.random.truncated_normal([Config.hidden_size],stddev=0.008), name='b_in')\n",
    "            \n",
    "            weights_h2 = tf.Variable(tf.random.truncated_normal([18*Config.embedding_size, Config.hidden_size], stddev=0.008), name='w_h2')\n",
    "            biases_h2 = tf.Variable(tf.random.truncated_normal([Config.hidden_size], stddev=0.008), name='b_h2')\n",
    "            \n",
    "            \n",
    "            weights_h3 = tf.Variable(tf.random.truncated_normal([12*Config.embedding_size, Config.hidden_size], stddev=0.008), name='w_h3')\n",
    "            biases_h3 = tf.Variable(tf.random.truncated_normal([Config.hidden_size], stddev=0.008), name='b_h3')\n",
    "\n",
    "            \n",
    "#             weights_h2 = tf.Variable(tf.random.truncated_normal([Config.hidden_size, Config.hidden_size*2], stddev=0.008), name='w_h2')\n",
    "#             biases_h2 = tf.Variable(tf.random.truncated_normal([Config.hidden_size*2], stddev=0.008), name='b_h2')\n",
    "            \n",
    "            \n",
    "#             weights_h3 = tf.Variable(tf.random.truncated_normal([Config.hidden_size*2, Config.hidden_size], stddev=0.008), name='w_h3')\n",
    "#             biases_h3 = tf.Variable(tf.random.truncated_normal([Config.hidden_size], stddev=0.008), name='b_h3')\n",
    "            \n",
    "            weights_output = tf.Variable(tf.random.truncated_normal([3*Config.hidden_size, numTrans], stddev=0.008), name='w_out')\n",
    "\n",
    "            self.prediction = self.forward_pass(self.train_embed, weights_input, biases_input, weights_output,    \n",
    "                                                weights_h2, biases_h2, weights_h3, biases_h3)\n",
    "\n",
    "            ### masked cross entropy loss\n",
    "            mask = self.train_labels > -1\n",
    "            mask = tf.dtypes.cast(mask,'int32')\n",
    "            self.labels = tf.math.multiply(self.train_labels,mask)\n",
    "            self.prediction = tf.math.multiply(self.prediction,tf.dtypes.cast(mask,'float'))\n",
    "            \n",
    "            self.labels = tf.math.argmax(self.train_labels,axis=1)\n",
    "            self.cce = tf.losses.sparse_softmax_cross_entropy(self.labels,self.prediction)\n",
    "\n",
    "            \n",
    "            self.regularization = tf.nn.l2_loss(weights_input) + tf.nn.l2_loss(biases_input) + tf.nn.l2_loss(weights_output)\\\n",
    "                                + tf.nn.l2_loss(self.train_embed)\\\n",
    "                                + tf.nn.l2_loss(weights_h2) + tf.nn.l2_loss(biases_h2)\\\n",
    "                                + tf.nn.l2_loss(weights_h3) + tf.nn.l2_loss(biases_h3)\n",
    "            \n",
    "            self.loss = self.cce + Config.lam * self.regularization\n",
    "            \n",
    "            ### optimizer optimization\n",
    "            global_step = tf.Variable(0,trainable=False)\n",
    "            learning_rate = tf.train.exponential_decay(Config.learning_rate,global_step=global_step, \n",
    "                                                       decay_steps=1000, decay_rate=0.75)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "#             optimizer = tf.train.GradientDescentOptimizer(Config.learning_rate)\n",
    "            \n",
    "            \n",
    "            grads = optimizer.compute_gradients(self.loss)\n",
    "            clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "            self.app = optimizer.apply_gradients(clipped_grads)\n",
    "\n",
    "            # For test data, we only need to get its prediction\n",
    "            self.test_inputs = tf.placeholder(\"int32\",[feature_len], name='test_input')\n",
    "            test_embed = tf.nn.embedding_lookup(self.embeddings, self.test_inputs)\n",
    "            test_embed = tf.reshape(test_embed, [1, -1], name='test_embed')\n",
    "            self.test_pred = self.forward_pass(test_embed, weights_input, biases_input, weights_output,  \n",
    "                                                weights_h2, biases_h2, weights_h3, biases_h3)\n",
    "\n",
    "            # intializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "    def train(self, sess, num_steps):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :param num_steps:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.init.run()\n",
    "        print \"Initailized\"\n",
    "        average_loss = 0\n",
    "        # import pdb; pdb.set_trace()\n",
    "        for step in range(num_steps):\n",
    "            start = (step * Config.batch_size) % len(trainFeats)\n",
    "            end = ((step + 1) * Config.batch_size) % len(trainFeats)\n",
    "            if end < start:\n",
    "                start -= end\n",
    "                end = len(trainFeats)\n",
    "            batch_inputs, batch_labels = trainFeats[start:end], trainLabels[start:end]\n",
    "\n",
    "            feed_dict = {self.train_inputs: batch_inputs, self.train_labels: batch_labels}\n",
    "\n",
    "            _, loss_val, cce, regu = sess.run([self.app, self.loss, self.cce, self.regularization], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % Config.display_step == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= Config.display_step\n",
    "#                 print(cce, Config.lam*regu)\n",
    "                print \"Average loss at step \", step, \": \", average_loss\n",
    "                average_loss = 0\n",
    "            if step % Config.validation_step == 0 and step != 0:\n",
    "                print \"\\nTesting on dev set at step \", step\n",
    "                predTrees = []\n",
    "                for sent in devSents:\n",
    "                    numTrans = parsing_system.numTransitions()\n",
    "\n",
    "                    c = parsing_system.initialConfiguration(sent)\n",
    "                    while not parsing_system.isTerminal(c):\n",
    "                        feat = getFeatures(c)\n",
    "                        pred = sess.run(self.test_pred, feed_dict={self.test_inputs: feat})\n",
    "\n",
    "                        optScore = -float('inf')\n",
    "                        optTrans = \"\"\n",
    "\n",
    "                        for j in range(numTrans):\n",
    "                            if pred[0, j] > optScore and parsing_system.canApply(c, parsing_system.transitions[j]):\n",
    "                                optScore = pred[0, j]\n",
    "                                optTrans = parsing_system.transitions[j]\n",
    "\n",
    "                        c = parsing_system.apply(c, optTrans)\n",
    "\n",
    "                    predTrees.append(c.tree)\n",
    "                result = parsing_system.evaluate(devSents, predTrees, devTrees)\n",
    "                print result\n",
    "\n",
    "        print \"Train Finished.\"\n",
    "\n",
    "    def evaluate(self, sess, testSents):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        print \"Starting to predict on test set\"\n",
    "        predTrees = []\n",
    "        for sent in testSents:\n",
    "            numTrans = parsing_system.numTransitions()\n",
    "\n",
    "            c = parsing_system.initialConfiguration(sent)\n",
    "            while not parsing_system.isTerminal(c):\n",
    "                # feat = getFeatureArray(c)\n",
    "                feat = getFeatures(c)\n",
    "                pred = sess.run(self.test_pred, feed_dict={self.test_inputs: feat})\n",
    "\n",
    "                optScore = -float('inf')\n",
    "                optTrans = \"\"\n",
    "\n",
    "                for j in range(numTrans):\n",
    "                    if pred[0, j] > optScore and parsing_system.canApply(c, parsing_system.transitions[j]):\n",
    "                        optScore = pred[0, j]\n",
    "                        optTrans = parsing_system.transitions[j]\n",
    "\n",
    "                c = parsing_system.apply(c, optTrans)\n",
    "\n",
    "            predTrees.append(c.tree)\n",
    "        print \"Saved the test results.\"\n",
    "        Util.writeConll('result_test.conll', testSents, predTrees)\n",
    "\n",
    "\n",
    "#     def forward_pass(self, embed, weights_input, biases_input, weights_output, weights_h2, biases_h2, weights_h3, biases_h3):\n",
    "    def forward_pass(self, embed, weights_input, biases_input, weights_output):\n",
    "        \"\"\"\n",
    "\n",
    "        :param embed:\n",
    "        :param weights:\n",
    "        :param biases:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        =======================================================\n",
    "\n",
    "        Implement the forwrad pass described in\n",
    "        \"A Fast and Accurate Dependency Parser using Neural Networks\"(2014)\n",
    "\n",
    "        =======================================================\n",
    "        \"\"\"\n",
    "        embed = tf.reshape(embed,[embed.shape[0],-1])\n",
    "#         word_embed, pos_embed, deb_embed = tf.split(embed,\n",
    "#                                                     [18*Config.embedding_size, 18*Config.embedding_size, 12*Config.embedding_size],\n",
    "#                                                     axis=1)        \n",
    "        \n",
    "#         word_h = tf.add(tf.matmul(word_embed,weights_input), biases_input)\n",
    "#         pos_h = tf.add(tf.matmul(pos_embed,weights_h2), biases_h2)\n",
    "#         deb_h = tf.add(tf.matmul(deb_embed,weights_h3), biases_h3)\n",
    "        \n",
    "#         word_act = tf.pow(word_h,3)\n",
    "#         pos_act = tf.pow(pos_h,3)\n",
    "#         deb_act = tf.pow(deb_h,3)\n",
    "        \n",
    "\n",
    "#         word_act = tf.nn.relu(word_h)\n",
    "#         pos_act = tf.nn.relu(pos_h)\n",
    "#         deb_act = tf.nn.relu(deb_h)\n",
    "    \n",
    "        \n",
    "#         all_hidden = tf.concat([word_act,pos_act,deb_act],axis=1)\n",
    "        \n",
    "        hidden_vals = tf.add(tf.matmul(embed,weights_input), biases_input)\n",
    "\n",
    "        activation = tf.pow(hidden_vals,3)\n",
    "#         activation = tf.nn.relu(hidden_vals)\n",
    "#         activation = tf.nn.sigmoid(hidden_vals)\n",
    "#         activation = tf.nn.tanh(hidden_vals)\n",
    "        \n",
    "#         if weights_h2 is not None:\n",
    "#         h2 = tf.add(tf.matmul(activation,weights_h2), biases_h2)\n",
    "#         h2_activation = tf.pow(h2,3)\n",
    "#         h2_activation = tf.nn.relu(h2)\n",
    "#         h2_activation = tf.nn.sigmoid(h2)\n",
    "#         h2_activation = tf.nn.tanh(h2)\n",
    "\n",
    "#         h3 = tf.add(tf.matmul(h2_activation,weights_h3), biases_h3)\n",
    "#         h3_activation = tf.pow(h3,3)\n",
    "#         h3_activation = tf.nn.relu(h3)\n",
    "#         h3_activation = tf.nn.sigmoid(h3)\n",
    "\n",
    "\n",
    "    \n",
    "        output = tf.matmul(activation,weights_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Config' from 'Config.pyc'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initailized\n",
      "Average loss at step  0 :  4.512081623077393\n",
      "Average loss at step  100 :  1.9641833329200744\n",
      "Average loss at step  200 :  1.1102117496728896\n",
      "Average loss at step  300 :  0.9379332387447357\n",
      "Average loss at step  400 :  0.849403315782547\n",
      "Average loss at step  500 :  0.8090888881683349\n",
      "\n",
      "Testing on dev set at step  500\n",
      "UAS: 37.3033875913\n",
      "UASnoPunc: 39.1115130278\n",
      "LAS: 27.9008898971\n",
      "LASnoPunc: 29.4862374951\n",
      "\n",
      "UEM: 3.11764705882\n",
      "UEMnoPunc: 3.29411764706\n",
      "ROOT: 25.8235294118\n",
      "\n",
      "Average loss at step  600 :  0.7697517359256745\n",
      "Average loss at step  700 :  0.7458649331331253\n",
      "Average loss at step  800 :  0.720496638417244\n",
      "Average loss at step  900 :  0.7037935096025467\n",
      "Average loss at step  1000 :  0.6869562524557113\n",
      "\n",
      "Testing on dev set at step  1000\n",
      "UAS: 42.4009771419\n",
      "UASnoPunc: 44.3113095575\n",
      "LAS: 33.2776628362\n",
      "LASnoPunc: 34.748205505\n",
      "\n",
      "UEM: 4.23529411765\n",
      "UEMnoPunc: 4.47058823529\n",
      "ROOT: 32.0\n",
      "\n",
      "Average loss at step  1100 :  0.6715079349279404\n",
      "Average loss at step  1200 :  0.6614025956392289\n",
      "Average loss at step  1300 :  0.6458846026659012\n",
      "Average loss at step  1400 :  0.6371789705753327\n",
      "Average loss at step  1500 :  0.6191938257217408\n",
      "\n",
      "Testing on dev set at step  1500\n",
      "UAS: 46.3992820999\n",
      "UASnoPunc: 48.3496297971\n",
      "LAS: 37.1288979734\n",
      "LASnoPunc: 38.6395749732\n",
      "\n",
      "UEM: 4.64705882353\n",
      "UEMnoPunc: 4.94117647059\n",
      "ROOT: 36.1176470588\n",
      "\n",
      "Average loss at step  1600 :  0.6181886970996857\n",
      "Average loss at step  1700 :  0.5990205693244934\n",
      "Average loss at step  1800 :  0.6004154986143112\n",
      "Average loss at step  1900 :  0.5862515550851822\n",
      "Average loss at step  2000 :  0.586180631518364\n",
      "\n",
      "Testing on dev set at step  2000\n",
      "UAS: 49.5500660568\n",
      "UASnoPunc: 51.7181936359\n",
      "LAS: 40.1201485654\n",
      "LASnoPunc: 41.5898943085\n",
      "\n",
      "UEM: 4.88235294118\n",
      "UEMnoPunc: 5.35294117647\n",
      "ROOT: 40.9411764706\n",
      "\n",
      "Average loss at step  2100 :  0.5743038034439087\n",
      "Average loss at step  2200 :  0.5746878224611283\n",
      "Average loss at step  2300 :  0.5637646061182022\n",
      "Average loss at step  2400 :  0.5607538497447968\n",
      "Average loss at step  2500 :  0.5552846211194992\n",
      "\n",
      "Testing on dev set at step  2500\n",
      "UAS: 50.0859984545\n",
      "UASnoPunc: 52.06578873\n",
      "LAS: 41.3764738141\n",
      "LASnoPunc: 42.8163680552\n",
      "\n",
      "UEM: 5.29411764706\n",
      "UEMnoPunc: 5.70588235294\n",
      "ROOT: 41.5882352941\n",
      "\n",
      "Average loss at step  2600 :  0.5514357662200928\n",
      "Average loss at step  2700 :  0.5440567526221275\n",
      "Average loss at step  2800 :  0.5422389525175094\n",
      "Average loss at step  2900 :  0.536383051276207\n",
      "Average loss at step  3000 :  0.5301612240076065\n",
      "\n",
      "Testing on dev set at step  3000\n",
      "UAS: 50.6194381434\n",
      "UASnoPunc: 52.7101113435\n",
      "LAS: 41.7578582646\n",
      "LASnoPunc: 43.282654157\n",
      "\n",
      "UEM: 4.41176470588\n",
      "UEMnoPunc: 4.94117647059\n",
      "ROOT: 39.7647058824\n",
      "\n",
      "Average loss at step  3100 :  0.5295878931879997\n",
      "Average loss at step  3200 :  0.5223450767993927\n",
      "Average loss at step  3300 :  0.5236488205194473\n",
      "Average loss at step  3400 :  0.512998574078083\n",
      "Average loss at step  3500 :  0.5184937211871147\n",
      "\n",
      "Testing on dev set at step  3500\n",
      "UAS: 52.0627165541\n",
      "UASnoPunc: 54.1654891765\n",
      "LAS: 43.1512824987\n",
      "LASnoPunc: 44.5826032894\n",
      "\n",
      "UEM: 5.52941176471\n",
      "UEMnoPunc: 5.88235294118\n",
      "ROOT: 43.5882352941\n",
      "\n",
      "Average loss at step  3600 :  0.5070157352089882\n",
      "Average loss at step  3700 :  0.5108025592565536\n",
      "Average loss at step  3800 :  0.5010852324962616\n",
      "Average loss at step  3900 :  0.5033396714925766\n",
      "Average loss at step  4000 :  0.4973551180958748\n",
      "\n",
      "Testing on dev set at step  4000\n",
      "UAS: 52.279582222\n",
      "UASnoPunc: 54.2870061606\n",
      "LAS: 43.5800284169\n",
      "LASnoPunc: 44.828463234\n",
      "\n",
      "UEM: 5.17647058824\n",
      "UEMnoPunc: 5.76470588235\n",
      "ROOT: 42.3529411765\n",
      "\n",
      "Average loss at step  4100 :  0.49887321531772616\n",
      "Average loss at step  4200 :  0.49300740361213685\n",
      "Average loss at step  4300 :  0.4907563257217407\n",
      "Average loss at step  4400 :  0.49021179854869845\n",
      "Average loss at step  4500 :  0.48772679328918456\n",
      "\n",
      "Testing on dev set at step  4500\n",
      "UAS: 53.458633497\n",
      "UASnoPunc: 55.6152150568\n",
      "LAS: 44.4923598474\n",
      "LASnoPunc: 45.7977731306\n",
      "\n",
      "UEM: 5.58823529412\n",
      "UEMnoPunc: 6.23529411765\n",
      "ROOT: 44.0\n",
      "\n",
      "Average loss at step  4600 :  0.48306551784276963\n",
      "Average loss at step  4700 :  0.48257090389728546\n",
      "Average loss at step  4800 :  0.4805909377336502\n",
      "Average loss at step  4900 :  0.47629606395959856\n",
      "Average loss at step  5000 :  0.4771377444267273\n",
      "\n",
      "Testing on dev set at step  5000\n",
      "UAS: 53.2068699055\n",
      "UASnoPunc: 55.2789238682\n",
      "LAS: 44.469925468\n",
      "LASnoPunc: 45.755383485\n",
      "\n",
      "UEM: 6.11764705882\n",
      "UEMnoPunc: 6.41176470588\n",
      "ROOT: 43.5882352941\n",
      "\n",
      "Average loss at step  5100 :  0.47140569508075714\n",
      "Average loss at step  5200 :  0.4745098000764847\n",
      "Average loss at step  5300 :  0.46594744980335234\n",
      "Average loss at step  5400 :  0.4735180947184563\n",
      "Average loss at step  5500 :  0.46298315465450285\n",
      "\n",
      "Testing on dev set at step  5500\n",
      "UAS: 53.7627439739\n",
      "UASnoPunc: 55.8215113322\n",
      "LAS: 45.2077672807\n",
      "LASnoPunc: 46.4392697677\n",
      "\n",
      "UEM: 5.88235294118\n",
      "UEMnoPunc: 6.58823529412\n",
      "ROOT: 43.4117647059\n",
      "\n",
      "Train Finished.\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "model = DependencyParserModel(graph, embedding_array, Config)\n",
    "\n",
    "num_steps = Config.max_iter\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    model.train(sess, num_steps)\n",
    "\n",
    "#     model.evaluate(sess, testSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
