{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "from progressbar import ProgressBar\n",
    "import cPickle as pkl\n",
    "\n",
    "from DependencyTree import DependencyTree\n",
    "from ParsingSystem import ParsingSystem\n",
    "from Configuration import Configuration\n",
    "import Config\n",
    "import Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-174-8fcaa7bd5bee>, line 199)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-174-8fcaa7bd5bee>\"\u001b[0;36m, line \u001b[0;32m199\u001b[0m\n\u001b[0;31m    for unknown words, pos and labels\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def genDictionaries(sents, trees):\n",
    "    word = []\n",
    "    pos = []\n",
    "    label = []\n",
    "    for s in sents:\n",
    "        for token in s:\n",
    "            word.append(token['word'])\n",
    "            pos.append(token['POS'])\n",
    "\n",
    "    rootLabel = None\n",
    "    for tree in trees:\n",
    "        for k in range(1, tree.n + 1):\n",
    "            if tree.getHead(k) == 0:\n",
    "                rootLabel = tree.getLabel(k)\n",
    "            else:\n",
    "                label.append(tree.getLabel(k))\n",
    "\n",
    "    if rootLabel in label:\n",
    "        label.remove(rootLabel)\n",
    "\n",
    "    index = 0\n",
    "    wordCount = [Config.UNKNOWN, Config.NULL, Config.ROOT]\n",
    "    wordCount.extend(collections.Counter(word))\n",
    "    for word in wordCount:\n",
    "        wordDict[word] = index\n",
    "        index += 1\n",
    "\n",
    "    posCount = [Config.UNKNOWN, Config.NULL, Config.ROOT]\n",
    "    posCount.extend(collections.Counter(pos))\n",
    "    for pos in posCount:\n",
    "        posDict[pos] = index\n",
    "        index += 1\n",
    "\n",
    "    labelCount = [Config.NULL, rootLabel]\n",
    "    labelCount.extend(collections.Counter(label))\n",
    "    for label in labelCount:\n",
    "        labelDict[label] = index\n",
    "        index += 1\n",
    "\n",
    "    return wordDict, posDict, labelDict\n",
    "\n",
    "\n",
    "def getWordID(s):\n",
    "    if s in wordDict:\n",
    "        return wordDict[s]\n",
    "    else:\n",
    "        return wordDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getPosID(s):\n",
    "    if s in posDict:\n",
    "        return posDict[s]\n",
    "    else:\n",
    "        return posDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getLabelID(s):\n",
    "    if s in labelDict:\n",
    "        return labelDict[s]\n",
    "    else:\n",
    "        return labelDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getFeatures(c):\n",
    "\n",
    "    \"\"\"\n",
    "    =================================================================\n",
    "\n",
    "    Implement feature extraction described in\n",
    "    \"A Fast and Accurate Dependency Parser using Neural Networks\"(2014)\n",
    "\n",
    "    =================================================================\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for i in range(3):\n",
    "        word_ind = c.getStack(i)\n",
    "        word = c.getWord(word_ind)\n",
    "        word_id = getWordID(word)\n",
    "\n",
    "        features.append(word_id)\n",
    "    \n",
    "    for i in range(3):\n",
    "        word_ind = c.getBuffer(i)\n",
    "        word = c.getWord(word_ind)\n",
    "        word_id = getWordID(word)\n",
    "\n",
    "        features.append(word_id)\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(1,3):\n",
    "            word_ind = c.getLeftChild(c.getStack(i),j)\n",
    "            word = c.getWord(word_ind)\n",
    "            word_id = getWordID(word)\n",
    "\n",
    "            features.append(word_id)\n",
    "\n",
    "            word_ind = c.getRightChild(c.getStack(i),j)\n",
    "            word = c.getWord(word_ind)\n",
    "            word_id = getWordID(word)\n",
    "\n",
    "            features.append(word_id)\n",
    "\n",
    "    for i in range(2):\n",
    "        word_ind = c.getLeftChild(c.getLeftChild(c.getStack(i),1),1)\n",
    "\n",
    "        word = c.getWord(word_ind)\n",
    "        word_id = getWordID(word)\n",
    "\n",
    "        features.append(word_id)\n",
    "\n",
    "        word_ind = c.getRightChild(c.getRightChild(c.getStack(i),1),1)\n",
    "\n",
    "        word = c.getWord(word_ind)\n",
    "        word_id = getWordID(word)\n",
    "\n",
    "        features.append(word_id)\n",
    "\n",
    "    pos_features = []\n",
    "    for ind in features:\n",
    "        pos = c.getPOS(ind)\n",
    "        pos_id = getPosID(pos)\n",
    "\n",
    "        pos_features.append(pos_id)\n",
    "\n",
    "    label_features = []\n",
    "    for ind in features[6:]: # excluding the 3 words from the top of stack and top of buffer\n",
    "        label = c.getLabel(ind)\n",
    "        if label == Config.UNKNOWN:\n",
    "            label = Config.NULL\n",
    "        label_id = getLabelID(label)\n",
    "\n",
    "        label_features.append(label_id)\n",
    "    \n",
    "    features.extend(pos_features)\n",
    "    features.extend(label_features)\n",
    "\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def genTrainExamples(sents, trees):\n",
    "    numTrans = parsing_system.numTransitions()\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    pbar = ProgressBar()\n",
    "    for i in pbar(range(len(sents))):\n",
    "    # for i in pbar(range(11)):\n",
    "\n",
    "        if trees[i].isProjective():\n",
    "            c = parsing_system.initialConfiguration(sents[i])\n",
    "\n",
    "            while not parsing_system.isTerminal(c):\n",
    "                oracle = parsing_system.getOracle(c, trees[i])\n",
    "                feat = getFeatures(c)\n",
    "                label = []\n",
    "                for j in range(numTrans):\n",
    "                    t = parsing_system.transitions[j]\n",
    "                    if t == oracle:\n",
    "                        label.append(1.)\n",
    "                    elif parsing_system.canApply(c, t):\n",
    "                        label.append(0.)\n",
    "                    else:\n",
    "                        label.append(-1.)\n",
    "\n",
    "                if 1.0 not in label:\n",
    "                    print i, label\n",
    "                features.append(feat)\n",
    "                labels.append(label)\n",
    "                c = parsing_system.apply(c, oracle)\n",
    "            \n",
    "#             print(trees[i].equal(c.tree))\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def load_embeddings(filename, wordDict, posDict, labelDict):\n",
    "    dictionary, word_embeds = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    embedding_array = np.zeros((len(wordDict) + len(posDict) + len(labelDict), Config.embedding_size))\n",
    "    pos_ind = 0\n",
    "    dep_ind = 0\n",
    "    knownWords = wordDict.keys()\n",
    "    foundEmbed = 0\n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        if i < len(knownWords):\n",
    "            w = knownWords[i]\n",
    "            if w in dictionary:\n",
    "                index = dictionary[w]\n",
    "            elif w.lower() in dictionary:\n",
    "                index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "\n",
    "        else:\n",
    "            for unknown words, pos and labels\n",
    "            embedding_array[i] = np.random.rand(Config.embedding_size) * 0.02 - 0.01            \n",
    "          \n",
    "#         elif i < len(wordDict):\n",
    "#             #for unknown words\n",
    "#             embedding_array[i] = np.random.rand(Config.embedding_size) * 0.02 - 0.01            \n",
    "        \n",
    "#         elif pos_ind < len(posDict):\n",
    "#             dummy_emb = [0 for _ in range(Config.embedding_size)]\n",
    "#             dummy_emb[pos_ind] = 1\n",
    "#             embedding_array[i] = dummy_emb[:]\n",
    "#             pos_ind+=1\n",
    "#         else:\n",
    "#             dummy_emb = [0 for _ in range(Config.embedding_size)]\n",
    "#             dummy_emb[dep_ind] = 1\n",
    "#             embedding_array[i] = dummy_emb[:]\n",
    "#             dep_ind+=1\n",
    "            \n",
    "    print \"Found embeddings: \", foundEmbed, \"/\", len(knownWords)\n",
    "\n",
    "    return embedding_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDict = {}\n",
    "posDict = {}\n",
    "labelDict = {}\n",
    "parsing_system = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings:  30160 / 44392\n"
     ]
    }
   ],
   "source": [
    "trainSents, trainTrees = Util.loadConll('train.conll')\n",
    "devSents, devTrees = Util.loadConll('dev.conll')\n",
    "testSents, _ = Util.loadConll('test.conll')\n",
    "genDictionaries(trainSents, trainTrees)\n",
    "\n",
    "embedding_filename = 'word2vec.model'\n",
    "\n",
    "embedding_array = load_embeddings(embedding_filename, wordDict, posDict, labelDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "Generating Training Examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "labelInfo = []\n",
    "for idx in np.argsort(labelDict.values()):\n",
    "    labelInfo.append(labelDict.keys()[idx])\n",
    "parsing_system = ParsingSystem(labelInfo[1:])\n",
    "print parsing_system.rootLabel\n",
    "\n",
    "print \"Generating Training Examples\"\n",
    "trainFeats, trainLabels = genTrainExamples(trainSents, trainTrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script defines a transition-based dependency parser which makes\n",
    "use of a classifier powered by a neural network. The neural network\n",
    "accepts distributed representation inputs: dense, continuous\n",
    "representations of words, their part of speech tags, and the labels\n",
    "which connect words in a partial dependency parse.\n",
    "\n",
    "This is an implementation of the method described in\n",
    "\n",
    "Danqi Chen and Christopher Manning. A Fast and Accurate Dependency Parser Using Neural Networks. In EMNLP 2014.\n",
    "\n",
    "Author: Danqi Chen, Jon Gauthier\n",
    "Modified by: Heeyoung Kwon (2017)\n",
    "Modified by: Jun S. Kang (2018 Mar)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DependencyParserModel(object):\n",
    "\n",
    "    def __init__(self, graph, embedding_array, Config):\n",
    "\n",
    "        self.build_graph(graph, embedding_array, Config)\n",
    "\n",
    "    def build_graph(self, graph, embedding_array, Config):\n",
    "        \"\"\"\n",
    "\n",
    "        :param graph:\n",
    "        :param embedding_array:\n",
    "        :param Config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        with graph.as_default():\n",
    "            \"\"\"\n",
    "            ===================================================================\n",
    "\n",
    "            Define the computational graph with necessary variables.\n",
    "            \n",
    "            1) You may need placeholders of:\n",
    "                - Many parameters are defined at Config: batch_size, n_Tokens, etc\n",
    "                - # of transitions can be get by calling parsing_system.numTransitions()\n",
    "                \n",
    "            self.train_inputs = \n",
    "            self.train_labels = \n",
    "            self.test_inputs =\n",
    "            ...\n",
    "            \n",
    "                \n",
    "            2) Call forward_pass and get predictions\n",
    "            \n",
    "            ...\n",
    "            self.prediction = self.forward_pass(embed, weights_input, biases_input, weights_output)\n",
    "\n",
    "\n",
    "            3) Implement the loss function described in the paper\n",
    "             - lambda is defined at Config.lam\n",
    "            \n",
    "            ...\n",
    "            self.loss =\n",
    "            \n",
    "            ==\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            self.embeddings = tf.Variable(embedding_array, dtype=tf.float32,name='all_embeddings', trainable=True)\n",
    "\n",
    "            feature_len = 18 + 18 + 12\n",
    "            numTrans = parsing_system.numTransitions()\n",
    "\n",
    "            self.train_inputs = tf.placeholder(\"int32\",[Config.batch_size,feature_len], name= 'train_input')\n",
    "            self.train_labels = tf.placeholder(\"int32\",[Config.batch_size,numTrans], name= 'train_labels')\n",
    "            self.train_embed = tf.nn.embedding_lookup(self.embeddings, self.train_inputs, name = 'train_embed')\n",
    "\n",
    "            \n",
    "#             ###### for best configuration ######\n",
    "#             weights_input = tf.Variable(tf.random.truncated_normal([18*Config.embedding_size, Config.hidden_size],stddev=0.08), name='w_in')\n",
    "#             biases_input = tf.Variable(tf.random.truncated_normal([Config.hidden_size],stddev=0.08), name='b_in')\n",
    "            \n",
    "#             weights_h2 = tf.Variable(tf.random.truncated_normal([18*Config.embedding_size, Config.hidden_size], stddev=0.08), name='w_h2')\n",
    "#             biases_h2 = tf.Variable(tf.random.truncated_normal([Config.hidden_size], stddev=0.08), name='b_h2')\n",
    "            \n",
    "            \n",
    "#             weights_h3 = tf.Variable(tf.random.truncated_normal([12*Config.embedding_size, Config.hidden_size], stddev=0.008), name='w_h3')\n",
    "#             biases_h3 = tf.Variable(tf.random.truncated_normal([Config.hidden_size], stddev=0.08), name='b_h3')\n",
    "\n",
    "#             weights_output = tf.Variable(tf.random.truncated_normal([3*Config.hidden_size, numTrans], stddev=0.08), name='w_out')\n",
    "\n",
    "#             self.prediction = self.forward_pass(self.train_embed, weights_input, biases_input, weights_output,    \n",
    "#                                                 weights_h2, biases_h2, weights_h3, biases_h3)\n",
    "            \n",
    "#             ### masked cross entropy loss\n",
    "#             mask = self.train_labels > -1\n",
    "#             mask = tf.dtypes.cast(mask,'int32')\n",
    "#             self.labels = tf.math.multiply(self.train_labels,mask)\n",
    "#             self.prediction = tf.math.multiply(self.prediction,tf.dtypes.cast(mask,'float'))\n",
    "            \n",
    "#             self.labels = tf.math.argmax(self.train_labels,axis=1)\n",
    "#             self.cce = tf.losses.sparse_softmax_cross_entropy(self.labels,self.prediction)\n",
    "\n",
    "            \n",
    "#             self.regularization = tf.nn.l2_loss(weights_input) + tf.nn.l2_loss(biases_input) + tf.nn.l2_loss(weights_output)\\\n",
    "#                                 + tf.nn.l2_loss(self.train_embed)\n",
    "#                                 + tf.nn.l2_loss(weights_h2) + tf.nn.l2_loss(biases_h2)\\\n",
    "#                                 + tf.nn.l2_loss(weights_h3) + tf.nn.l2_loss(biases_h3)\n",
    "            \n",
    "#             self.loss = self.cce + Config.lam * self.regularization\n",
    "            \n",
    "#             optimizer = tf.train.AdamOptimizer(Config.learning_rate)\n",
    "\n",
    "            \n",
    "#             grads = optimizer.compute_gradients(self.loss)\n",
    "#             clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "#             self.app = optimizer.apply_gradients(clipped_grads)            \n",
    "            \n",
    "#             ###### for best configuration ######\n",
    "            \n",
    "            ### multiple hidden layer experiments ###\n",
    "#             weights_h2 = tf.Variable(tf.random.truncated_normal([Config.hidden_size, Config.hidden_size], stddev=0.08), name='w_h2')\n",
    "#             biases_h2 = tf.Variable(tf.random.truncated_normal([Config.hidden_size], stddev=0.08), name='b_h2')\n",
    "            \n",
    "            \n",
    "#             weights_h3 = tf.Variable(tf.random.truncated_normal([Config.hidden_size, Config.hidden_size], stddev=0.08), name='w_h3')\n",
    "#             biases_h3 = tf.Variable(tf.random.truncated_normal([Config.hidden_size], stddev=0.08), name='b_h3')\n",
    "            ### multiple hidden layers experiments ###\n",
    "    \n",
    "    \n",
    "            ####### default configuration #######\n",
    "            weights_input = tf.Variable(tf.random.truncated_normal([feature_len*Config.embedding_size, Config.hidden_size],stddev=0.08), name='w_in')\n",
    "            biases_input = tf.Variable(tf.random.truncated_normal([Config.hidden_size],stddev=0.08), name='b_in')\n",
    "\n",
    "            weights_output = tf.Variable(tf.random.truncated_normal([Config.hidden_size, numTrans], stddev=0.08), name='w_out')\n",
    "\n",
    "            self.prediction = self.forward_pass(self.train_embed, weights_input, biases_input, weights_output)    \n",
    "            \n",
    "            self.labels = tf.math.argmax(self.train_labels,axis=1)\n",
    "            self.cce = tf.losses.sparse_softmax_cross_entropy(self.labels,self.prediction)\n",
    "\n",
    "            \n",
    "            self.regularization = tf.nn.l2_loss(weights_input) + tf.nn.l2_loss(biases_input) + tf.nn.l2_loss(weights_output)\\\n",
    "                                + tf.nn.l2_loss(self.train_embed)\n",
    "            \n",
    "            self.loss = self.cce + Config.lam * self.regularization\n",
    "            optimizer = tf.train.GradientDescentOptimizer(Config.learning_rate)\n",
    "\n",
    "\n",
    "            grads = optimizer.compute_gradients(self.loss)\n",
    "#             clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "            self.app = optimizer.apply_gradients(grads)\n",
    "\n",
    "            \n",
    "\n",
    "            ####### default configuration #######\n",
    "            \n",
    "\n",
    "            \n",
    "            # For test data, we only need to get its prediction\n",
    "            self.test_inputs = tf.placeholder(\"int32\",[feature_len], name='test_input')\n",
    "            test_embed = tf.nn.embedding_lookup(self.embeddings, self.test_inputs)\n",
    "            test_embed = tf.reshape(test_embed, [1, -1], name='test_embed')\n",
    "            \n",
    "#             ###### for best configuration ######\n",
    "#             self.test_pred = self.forward_pass(test_embed, weights_input, biases_input, weights_output,\n",
    "#                                                weights_h2, biases_h2, weights_h3, biases_h3)\n",
    "\n",
    "#             ###### for best configuration ######\n",
    "            \n",
    "            ####### default configuration #######\n",
    "            self.test_pred = self.forward_pass(test_embed, weights_input, biases_input, weights_output)\n",
    "            ####### default configuration #######\n",
    "            \n",
    "            # intializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "    def train(self, sess, num_steps):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :param num_steps:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.init.run()\n",
    "        print \"Initailized\"\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            start = (step * Config.batch_size) % len(trainFeats)\n",
    "            end = ((step + 1) * Config.batch_size) % len(trainFeats)\n",
    "            if end < start:\n",
    "                start -= end\n",
    "                end = len(trainFeats)\n",
    "            batch_inputs, batch_labels = trainFeats[start:end], trainLabels[start:end]\n",
    "\n",
    "            feed_dict = {self.train_inputs: batch_inputs, self.train_labels: batch_labels}\n",
    "\n",
    "            _, loss_val, cce, regu = sess.run([self.app, self.loss, self.cce, self.regularization], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % Config.display_step == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= Config.display_step\n",
    "                print \"Average loss at step \", step, \": \", average_loss\n",
    "                average_loss = 0\n",
    "            if step % Config.validation_step == 0 and step != 0:\n",
    "                print \"\\nTesting on dev set at step \", step\n",
    "                predTrees = []\n",
    "                for sent in devSents:\n",
    "                    numTrans = parsing_system.numTransitions()\n",
    "\n",
    "                    c = parsing_system.initialConfiguration(sent)\n",
    "                    while not parsing_system.isTerminal(c):\n",
    "                        feat = getFeatures(c)\n",
    "                        pred = sess.run(self.test_pred, feed_dict={self.test_inputs: feat})\n",
    "\n",
    "                        optScore = -float('inf')\n",
    "                        optTrans = \"\"\n",
    "\n",
    "                        for j in range(numTrans):\n",
    "                            if pred[0, j] > optScore and parsing_system.canApply(c, parsing_system.transitions[j]):\n",
    "                                optScore = pred[0, j]\n",
    "                                optTrans = parsing_system.transitions[j]\n",
    "\n",
    "                        c = parsing_system.apply(c, optTrans)\n",
    "\n",
    "                    predTrees.append(c.tree)\n",
    "                result = parsing_system.evaluate(devSents, predTrees, devTrees)\n",
    "                print result\n",
    "\n",
    "        print \"Train Finished.\"\n",
    "\n",
    "    def evaluate(self, sess, testSents):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        print \"Starting to predict on test set\"\n",
    "        predTrees = []\n",
    "        for sent in testSents:\n",
    "            numTrans = parsing_system.numTransitions()\n",
    "\n",
    "            c = parsing_system.initialConfiguration(sent)\n",
    "            while not parsing_system.isTerminal(c):\n",
    "                # feat = getFeatureArray(c)\n",
    "                feat = getFeatures(c)\n",
    "                pred = sess.run(self.test_pred, feed_dict={self.test_inputs: feat})\n",
    "\n",
    "                optScore = -float('inf')\n",
    "                optTrans = \"\"\n",
    "\n",
    "                for j in range(numTrans):\n",
    "                    if pred[0, j] > optScore and parsing_system.canApply(c, parsing_system.transitions[j]):\n",
    "                        optScore = pred[0, j]\n",
    "                        optTrans = parsing_system.transitions[j]\n",
    "\n",
    "                c = parsing_system.apply(c, optTrans)\n",
    "\n",
    "            predTrees.append(c.tree)\n",
    "        print \"Saved the test results.\"\n",
    "        Util.writeConll('result_test.conll', testSents, predTrees)\n",
    "\n",
    "\n",
    "#     def forward_pass(self, embed, weights_input, biases_input, weights_output, weights_h2, biases_h2, weights_h3, biases_h3):\n",
    "    def forward_pass(self, embed, weights_input, biases_input, weights_output):\n",
    "        \"\"\"\n",
    "        :param embed:\n",
    "        :param weights:\n",
    "        :param biases:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        =======================================================\n",
    "\n",
    "        Implement the forwrad pass described in\n",
    "        \"A Fast and Accurate Dependency Parser using Neural Networks\"(2014)\n",
    "\n",
    "        =======================================================\n",
    "        \"\"\"\n",
    "        embed = tf.reshape(embed,[embed.shape[0],-1])\n",
    "        \n",
    "        ######## For best configuration ##########\n",
    "#         word_embed, pos_embed, deb_embed = tf.split(embed,\n",
    "#                                                     [18*Config.embedding_size, 18*Config.embedding_size, 12*Config.embedding_size],\n",
    "#                                                     axis=1)        \n",
    "        \n",
    "#         word_h = tf.add(tf.matmul(word_embed,weights_input), biases_input)\n",
    "#         pos_h = tf.add(tf.matmul(pos_embed,weights_h2), biases_h2)\n",
    "#         deb_h = tf.add(tf.matmul(deb_embed,weights_h3), biases_h3)\n",
    "        \n",
    "#         # cube activation function for experiment 2.b\n",
    "# #         word_act = tf.pow(word_h,3)\n",
    "# #         pos_act = tf.pow(pos_h,3)\n",
    "# #         deb_act = tf.pow(deb_h,3)\n",
    "        \n",
    "\n",
    "#         word_act = tf.nn.relu(word_h)\n",
    "#         pos_act = tf.nn.relu(pos_h)\n",
    "#         deb_act = tf.nn.relu(deb_h)\n",
    "    \n",
    "        \n",
    "#         all_hidden = tf.concat([word_act,pos_act,deb_act],axis=1)\n",
    "# #         all_hidden = tf.math.multiply(tf.math.multiply(word_act,pos_act),deb_act )\n",
    "                \n",
    "#         output = tf.matmul(all_hidden,weights_output)\n",
    "        ######## For best configuration ##########\n",
    "        \n",
    "        ###### For default model + commented out parts for accomodating multiple layers ######\n",
    "\n",
    "        hidden_vals = tf.add(tf.matmul(embed,weights_input), biases_input)\n",
    "\n",
    "\n",
    "        ## options of different activation functions ###\n",
    "        activation = tf.pow(hidden_vals,3)\n",
    "#         activation = tf.nn.relu(hidden_vals)\n",
    "#         activation = tf.nn.sigmoid(hidden_vals)\n",
    "#         activation = tf.nn.tanh(hidden_vals)\n",
    "        \n",
    "#         # second hidden layer ##\n",
    "#         h2 = tf.add(tf.matmul(activation,weights_h2), biases_h2)\n",
    "#         h2_activation = tf.pow(h2,3)\n",
    "#         h2_activation = tf.nn.relu(h2)\n",
    "#         h2_activation = tf.nn.sigmoid(h2)\n",
    "#         h2_activation = tf.nn.tanh(h2)\n",
    "#         # second hidden layer ##\n",
    "    \n",
    "        # third hidden layer ##\n",
    "#         h3 = tf.add(tf.matmul(h2_activation,weights_h3), biases_h3)\n",
    "#         h3_activation = tf.pow(h3,3)\n",
    "#         h3_activation = tf.nn.relu(h3)\n",
    "#         h3_activation = tf.nn.sigmoid(h3)\n",
    "#         # third hidden layer ##\n",
    "\n",
    "        output = tf.matmul(activation,weights_output)\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Config' from 'Config.pyc'>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initailized\n",
      "Average loss at step  0 :  4.502429962158203\n",
      "Average loss at step  100 :  2.3496189200878144\n",
      "Average loss at step  200 :  1.5714241325855256\n",
      "Average loss at step  300 :  1.359567675590515\n",
      "Average loss at step  400 :  1.1980072784423828\n",
      "Average loss at step  500 :  1.0928677105903626\n",
      "\n",
      "Testing on dev set at step  500\n",
      "UAS: 28.2972306005\n",
      "UASnoPunc: 29.8875261403\n",
      "LAS: 18.0821098288\n",
      "LASnoPunc: 19.2081614198\n",
      "\n",
      "UEM: 1.64705882353\n",
      "UEMnoPunc: 1.64705882353\n",
      "ROOT: 6.47058823529\n",
      "\n",
      "Average loss at step  600 :  1.0029789751768112\n",
      "Average loss at step  700 :  0.9417948865890503\n",
      "Average loss at step  800 :  0.8847065669298172\n",
      "Average loss at step  900 :  0.8413177263736725\n",
      "Average loss at step  1000 :  0.8028719049692153\n",
      "\n",
      "Testing on dev set at step  1000\n",
      "UAS: 39.708851609\n",
      "UASnoPunc: 42.0505284576\n",
      "LAS: 29.4887454196\n",
      "LASnoPunc: 31.0603063358\n",
      "\n",
      "UEM: 2.11764705882\n",
      "UEMnoPunc: 2.29411764706\n",
      "ROOT: 17.4705882353\n",
      "\n",
      "Train Finished.\n",
      "Starting to predict on test set\n",
      "Saved the test results.\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "model = DependencyParserModel(graph, embedding_array, Config)\n",
    "\n",
    "num_steps = Config.max_iter\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    model.train(sess, num_steps)\n",
    "\n",
    "    model.evaluate(sess, testSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
